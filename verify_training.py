#!/usr/bin/env python3
"""
Script para verificar que el entrenamiento se completÃ³ correctamente
"""

import os
import json
import glob
from datetime import datetime

def verify_training_completion():
    """Verificar que todos los archivos esperados existan y sean vÃ¡lidos"""
    
    print("ğŸ” VERIFICACIÃ“N DE ENTRENAMIENTO")
    print("=" * 40)
    
    checks = []
    
    # 1. Verificar modelos guardados
    model_files = glob.glob("models/saved_models/*.h5")
    metadata_file = "models/saved_models/model_metadata.json"
    
    if model_files:
        latest_model = max(model_files, key=os.path.getctime)
        checks.append(("âœ…", f"Modelo encontrado: {os.path.basename(latest_model)}"))
        
        if os.path.exists(metadata_file):
            checks.append(("âœ…", "Metadata del modelo existe"))
        else:
            checks.append(("âŒ", "Metadata del modelo NO encontrada"))
    else:
        checks.append(("âŒ", "NO se encontrÃ³ modelo entrenado"))
    
    # 2. Verificar checkpoints
    checkpoint_files = glob.glob("models/checkpoints/*.h5")
    if checkpoint_files:
        checks.append(("âœ…", f"Checkpoints encontrados: {len(checkpoint_files)}"))
    else:
        checks.append(("âš ï¸", "No se encontraron checkpoints"))
    
    # 3. Verificar resultados
    results_file = "results/evaluation_results.json"
    if os.path.exists(results_file):
        checks.append(("âœ…", "Archivo de resultados existe"))
        
        try:
            with open(results_file, 'r') as f:
                results = json.load(f)
            
            accuracy = results.get('test_accuracy', 0)
            checks.append(("ğŸ“Š", f"PrecisiÃ³n en test: {accuracy:.4f} ({accuracy*100:.1f}%)"))
            
            if accuracy > 0.8:
                checks.append(("ğŸ‰", "Excelente precisiÃ³n!"))
            elif accuracy > 0.6:
                checks.append(("ğŸ‘", "Buena precisiÃ³n"))
            else:
                checks.append(("âš ï¸", "PrecisiÃ³n baja - considera mÃ¡s entrenamiento"))
                
        except Exception as e:
            checks.append(("âŒ", f"Error leyendo resultados: {e}"))
    else:
        checks.append(("âŒ", "Archivo de resultados NO encontrado"))
    
    # 4. Verificar grÃ¡ficos
    plots_dir = "results/plots"
    if os.path.exists(plots_dir):
        plot_files = os.listdir(plots_dir)
        if len(plot_files) > 0:
            checks.append(("âœ…", f"GrÃ¡ficos generados: {len(plot_files)}"))
        else:
            checks.append(("âš ï¸", "Carpeta de grÃ¡ficos vacÃ­a"))
    else:
        checks.append(("âŒ", "Carpeta de grÃ¡ficos NO existe"))
    
    # 5. Verificar logs de entrenamiento
    log_files = glob.glob("results/training_log_*.csv")
    if log_files:
        checks.append(("âœ…", "Logs de entrenamiento encontrados"))
    else:
        checks.append(("âš ï¸", "No se encontraron logs de entrenamiento"))
    
    # 6. Verificar estructura de datos
    data_dirs = ["dataset/train", "dataset/validation", "dataset/test"]
    all_data_ok = True
    for dir_path in data_dirs:
        if os.path.exists(dir_path):
            subdirs = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]
            if len(subdirs) == 5:  # 5 personajes
                checks.append(("âœ…", f"{dir_path}: {len(subdirs)} clases"))
            else:
                checks.append(("âš ï¸", f"{dir_path}: Solo {len(subdirs)} clases"))
                all_data_ok = False
        else:
            checks.append(("âŒ", f"{dir_path}: NO existe"))
            all_data_ok = False
    
    # Mostrar resultados
    print("\nğŸ“‹ RESULTADOS DE VERIFICACIÃ“N:")
    for icon, message in checks:
        print(f"{icon} {message}")
    
    # Resumen final
    errors = sum(1 for icon, _ in checks if icon == "âŒ")
    warnings = sum(1 for icon, _ in checks if icon == "âš ï¸")
    successes = sum(1 for icon, _ in checks if icon == "âœ…")
    
    print(f"\nğŸ“Š RESUMEN:")
    print(f"âœ… Verificaciones exitosas: {successes}")
    print(f"âš ï¸  Advertencias: {warnings}")
    print(f"âŒ Errores: {errors}")
    
    if errors == 0:
        if warnings == 0:
            print(f"\nğŸ‰ Â¡TODO PERFECTO! El entrenamiento se completÃ³ exitosamente.")
        else:
            print(f"\nğŸ‘ Entrenamiento completado con algunas advertencias menores.")
        
        print(f"\nğŸš€ TU MODELO ESTÃ LISTO PARA:")
        print(f"   - Hacer predicciones de nuevas imÃ¡genes")
        print(f"   - Subir a Google Colab")
        print(f"   - Integrar en aplicaciones web")
        print(f"   - Deployment en producciÃ³n")
        
    else:
        print(f"\nâš ï¸  Hay algunos problemas que necesitan atenciÃ³n.")
        print(f"   Revisa los errores marcados con âŒ")
    
    return errors == 0

def show_model_info():
    """Mostrar informaciÃ³n detallada del modelo"""
    
    metadata_file = "models/saved_models/model_metadata.json"
    
    if os.path.exists(metadata_file):
        print(f"\nğŸ“‹ INFORMACIÃ“N DEL MODELO:")
        print("-" * 30)
        
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        print(f"ğŸ·ï¸  Clases: {', '.join(metadata.get('class_names', []))}")
        print(f"ğŸ“ TamaÃ±o de imagen: {metadata.get('img_size', 'N/A')}")
        print(f"ğŸ“… Fecha de creaciÃ³n: {metadata.get('timestamp', 'N/A')}")
        print(f"ğŸ“ Archivo del modelo: {os.path.basename(metadata.get('model_path', 'N/A'))}")
    
    # Mostrar resultados de evaluaciÃ³n
    results_file = "results/evaluation_results.json"
    if os.path.exists(results_file):
        print(f"\nğŸ“Š MÃ‰TRICAS DE EVALUACIÃ“N:")
        print("-" * 30)
        
        with open(results_file, 'r') as f:
            results = json.load(f)
        
        print(f"ğŸ¯ PrecisiÃ³n: {results.get('test_accuracy', 0):.4f} ({results.get('test_accuracy', 0)*100:.1f}%)")
        print(f"ğŸ” Top-3 Accuracy: {results.get('test_top3_accuracy', 0):.4f}")
        print(f"ğŸ“‰ Loss: {results.get('test_loss', 0):.4f}")

def main():
    """FunciÃ³n principal"""
    
    training_success = verify_training_completion()
    
    if training_success:
        show_model_info()
        
        print(f"\nğŸ’¡ PRÃ“XIMOS PASOS:")
        print(f"   1. Probar predicciones: py scripts/test_prediction.py")
        print(f"   2. Ver grÃ¡ficos en: results/plots/")
        print(f"   3. Subir proyecto a Colab")
        print(f"   4. Crear aplicaciÃ³n de predicciÃ³n")

if __name__ == "__main__":
    main()